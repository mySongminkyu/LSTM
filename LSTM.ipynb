{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c3YkCFMxPFuz"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WNt06XY6kcY",
        "outputId": "be4adddf-eeaa-47d6-9e5b-00c605e8bcb4"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import time\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "kFrTjK2VPVHW"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리\n",
        "file_path = '/content/drive/MyDrive/LSTM DATA.txt'\n",
        "\n",
        "with open(file_path, 'r', encoding='utf8') as file:\n",
        "    lines = [line.strip() for line in file]\n",
        "\n",
        "data = ' '.join(lines)\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace('”', '')\n",
        "\n",
        "data = data.lower()\n",
        "words = list(set(data.split()))"
      ],
      "metadata": {
        "id": "bYLrQ47U6V9L"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaYL6qe48Rd4",
        "outputId": "09b0b818-868c-4823-f365-e74072e0f54a"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['be--if',\n",
              " 'half-reluctant',\n",
              " 'blushed,',\n",
              " 'ago.',\n",
              " 'patronage,',\n",
              " 'obsolete,',\n",
              " '_are_',\n",
              " 'add.',\n",
              " 'battled',\n",
              " 'demands',\n",
              " 'peep',\n",
              " '_knew_',\n",
              " '(trademark/copyright)',\n",
              " 'accompanying',\n",
              " 'softness',\n",
              " 'motive.',\n",
              " 'paining',\n",
              " 'performed',\n",
              " 'bingleys’',\n",
              " 'success,--no',\n",
              " 'grieved.',\n",
              " 'display.',\n",
              " 'adds,',\n",
              " 'enabling',\n",
              " 'displaying',\n",
              " 'ay,',\n",
              " 'afflicting',\n",
              " 'excelled',\n",
              " 'insensible',\n",
              " 'for,',\n",
              " 'support.',\n",
              " 'unnecessary.',\n",
              " 'lament',\n",
              " 'inhuman',\n",
              " 'less,',\n",
              " 'denying',\n",
              " 'edmund',\n",
              " 'earthly',\n",
              " 'animating',\n",
              " 'hurrying',\n",
              " 'thoughtless',\n",
              " 'refused?',\n",
              " 'otherwise,',\n",
              " 'lane,',\n",
              " 'resist',\n",
              " 'plain,',\n",
              " 'remark;',\n",
              " 'fine,--with',\n",
              " 'as,',\n",
              " 'neighbourhood.',\n",
              " 'improbable.',\n",
              " 'declaration.',\n",
              " 'vindication,',\n",
              " 'ponies',\n",
              " 'october_.',\n",
              " 'dinner:',\n",
              " '270',\n",
              " 'possession',\n",
              " 'residence',\n",
              " 'agreeable',\n",
              " 'gloomy.',\n",
              " 'ensued,',\n",
              " 'hesitate;',\n",
              " 'misleading',\n",
              " 'man--he',\n",
              " '53',\n",
              " 'replied',\n",
              " 'are)',\n",
              " 'dazzling',\n",
              " 'jones',\n",
              " 'mend',\n",
              " '18',\n",
              " 'belonged.',\n",
              " 'gallery.',\n",
              " '_mightily--might',\n",
              " 'come.',\n",
              " 'forster’s!',\n",
              " 'utmost.',\n",
              " 'heavier',\n",
              " 'illness--if',\n",
              " 'palatable',\n",
              " 'objection',\n",
              " 'discretion.',\n",
              " 'first',\n",
              " 'living',\n",
              " 'inconceivable',\n",
              " 'business',\n",
              " 'wanting.',\n",
              " 'vanish,',\n",
              " 'indefinite,',\n",
              " 'teasing',\n",
              " 'bottle',\n",
              " 'charles',\n",
              " 'suffer!',\n",
              " 'fee',\n",
              " 'happens.',\n",
              " 'sincere;',\n",
              " 'instrument,',\n",
              " 'former.',\n",
              " 'congratulations.',\n",
              " 'shows',\n",
              " 'forward.',\n",
              " '(801)',\n",
              " 'felicity?',\n",
              " 'womanly',\n",
              " 'was!',\n",
              " 'complaisance',\n",
              " 'running',\n",
              " 'individual',\n",
              " 'likelihood',\n",
              " 'by,',\n",
              " 'disappointment!',\n",
              " 'frequent.',\n",
              " 'approbation',\n",
              " 'renewal.',\n",
              " 'first,',\n",
              " 'despised',\n",
              " 'ran',\n",
              " 'immediate',\n",
              " 'begin.',\n",
              " 'benches,',\n",
              " 'fully.',\n",
              " 'bitterly',\n",
              " 'creditable,',\n",
              " 'business,',\n",
              " 'promised;',\n",
              " 'savagery',\n",
              " 'attracted',\n",
              " '_sentiment_',\n",
              " 'it--you',\n",
              " 'gutenberg™,',\n",
              " 'dressed',\n",
              " 'intelligence:',\n",
              " 'such,',\n",
              " 'her--indeed,',\n",
              " 'mutually',\n",
              " 'refund',\n",
              " 'humiliating',\n",
              " 'hasten',\n",
              " 'derive',\n",
              " '‘bingley,',\n",
              " 'hardly',\n",
              " 'occurring',\n",
              " 'tears,--an',\n",
              " 'copse;',\n",
              " 'equipment.',\n",
              " 'overpower',\n",
              " 'oppose',\n",
              " 'believe.',\n",
              " 'mind--pride',\n",
              " 'defective.',\n",
              " 'larger.',\n",
              " 'interesting.',\n",
              " 'heartening',\n",
              " 'prudent.',\n",
              " '_generous.',\n",
              " 'friend’s',\n",
              " 'same;',\n",
              " 'altogether:',\n",
              " 'road?',\n",
              " 'immediately',\n",
              " 'strange',\n",
              " 'accusations',\n",
              " 'dedication',\n",
              " 'invitation',\n",
              " 'bright',\n",
              " 'coolly',\n",
              " 'invalid,',\n",
              " 'calmness,',\n",
              " 'plentiful',\n",
              " 'distress;',\n",
              " 'ingenuity',\n",
              " 'comprise',\n",
              " 'pieces.',\n",
              " 'parlour,',\n",
              " 'knows',\n",
              " 'witnessing',\n",
              " 'cordiality',\n",
              " 'speeches',\n",
              " 'saturday,',\n",
              " 'follows:--',\n",
              " 'perfections.',\n",
              " 'separation.',\n",
              " 'blown',\n",
              " 'separating',\n",
              " 'them,',\n",
              " 'procuring',\n",
              " 'discretion;',\n",
              " 'impelled',\n",
              " 'wickham’s',\n",
              " 'similarity',\n",
              " 'stiffness',\n",
              " 'exceeded',\n",
              " 'well-educated',\n",
              " 'believe,',\n",
              " 'unsuccessfully',\n",
              " 'bore',\n",
              " 'odd!',\n",
              " 'defective,',\n",
              " 'forster?',\n",
              " 'terrific',\n",
              " 'consequences',\n",
              " 'materialism,',\n",
              " 'risk',\n",
              " 'regular',\n",
              " 'energy;',\n",
              " 'exposing',\n",
              " 'acceptable.',\n",
              " 'scornful',\n",
              " 'procure',\n",
              " 'employments,',\n",
              " 'sea-bathing',\n",
              " '1500',\n",
              " 'however;',\n",
              " 'else',\n",
              " 'eye;',\n",
              " 'exultation,',\n",
              " 'assembly!',\n",
              " 'earlier.',\n",
              " 'love.',\n",
              " 'consented,',\n",
              " 'denied',\n",
              " 'proud;',\n",
              " 'memory;',\n",
              " 'prettier',\n",
              " 'of;',\n",
              " 'negative.',\n",
              " 'tolerable,',\n",
              " 'mornings',\n",
              " 'displease',\n",
              " 'match:',\n",
              " 'wollstonecraft',\n",
              " 'disturbers',\n",
              " 'intention.',\n",
              " 'inadequate',\n",
              " 'surmises;',\n",
              " 'happiness?',\n",
              " 'equipment',\n",
              " 'bears',\n",
              " 'distinguishing',\n",
              " 'grievances',\n",
              " 'joy?',\n",
              " 'song.',\n",
              " 'false,',\n",
              " 'cold?',\n",
              " 'return,',\n",
              " 'diversion.',\n",
              " 'ours',\n",
              " 'requirements.',\n",
              " 'head-quarters.',\n",
              " 'principal,',\n",
              " 'was.',\n",
              " 'ceremony.',\n",
              " 'estimated',\n",
              " 'genius',\n",
              " 'believed.',\n",
              " 'untamed,',\n",
              " '_coming',\n",
              " 'fidgety',\n",
              " 'matter?',\n",
              " 'kind:',\n",
              " 'distressing,',\n",
              " 'introduced,',\n",
              " 'mating',\n",
              " 'possibility',\n",
              " 'house',\n",
              " 'greater--what',\n",
              " 'violated',\n",
              " 'temptation,',\n",
              " 'amaze',\n",
              " 'surprise;',\n",
              " 'it:--till',\n",
              " 'ramble,',\n",
              " 'dear',\n",
              " 'well',\n",
              " 'exciting',\n",
              " '359',\n",
              " 'bonnet.',\n",
              " 'esmond,',\n",
              " 'charms',\n",
              " 'horses,',\n",
              " 'notes',\n",
              " 'voice;',\n",
              " 'panegyric',\n",
              " 'after.',\n",
              " 'absurdities',\n",
              " 'feverish',\n",
              " 'vicinity',\n",
              " 'cared',\n",
              " 'mine;',\n",
              " 'assure',\n",
              " 'curious',\n",
              " 'staggered',\n",
              " 'exalt',\n",
              " 'profligacy',\n",
              " 'fees',\n",
              " 'words.',\n",
              " 'recalled',\n",
              " 'sacrificed',\n",
              " 'fast.',\n",
              " 'clothes!',\n",
              " 'enough;',\n",
              " 'remained,',\n",
              " 'insipidity,',\n",
              " 'officers’',\n",
              " 'sending',\n",
              " 'either;',\n",
              " 'assembled.',\n",
              " 'man,',\n",
              " 'puddles,',\n",
              " 'why',\n",
              " 'critics,',\n",
              " 'talked,',\n",
              " 'much.',\n",
              " 'dissatisfied,',\n",
              " 'defect',\n",
              " 'affectation',\n",
              " 'user,',\n",
              " 'chosen.',\n",
              " 'pretty;',\n",
              " 'direction',\n",
              " 'sigh.',\n",
              " 'unreasonably)',\n",
              " 'registered',\n",
              " 'blinded',\n",
              " 'glove',\n",
              " 'disengaged',\n",
              " 'nerves.',\n",
              " 'street,--and',\n",
              " 'connects',\n",
              " 'mind:',\n",
              " 'contrived',\n",
              " 'indulging',\n",
              " 'knowing',\n",
              " 'hurried',\n",
              " 'horrible!',\n",
              " 'already,',\n",
              " 'finest',\n",
              " 'annexed',\n",
              " 'justify',\n",
              " 'friend',\n",
              " 'gentleman.',\n",
              " 'mien,',\n",
              " 'place.',\n",
              " 'understand?',\n",
              " 'super-excellent',\n",
              " 'crowd,',\n",
              " 'per',\n",
              " 'went,',\n",
              " 'unless,',\n",
              " 'dear?',\n",
              " 'contents?',\n",
              " 'everybody--would',\n",
              " 'imposing',\n",
              " 'author’s',\n",
              " 'netherfield;',\n",
              " 'regard;',\n",
              " 'one-and-twenty.',\n",
              " 'self-conquest',\n",
              " 'beheld',\n",
              " 'presuming,',\n",
              " 'blind.',\n",
              " 'thanks.',\n",
              " 'xxviii.',\n",
              " 'recommended',\n",
              " 'nearer',\n",
              " 'serve',\n",
              " 'plague',\n",
              " 'same',\n",
              " 'intolerable,',\n",
              " 'biting',\n",
              " 'refrained',\n",
              " '1.e.1',\n",
              " 'serious.',\n",
              " 'insincere.',\n",
              " 'true;',\n",
              " 'chagrin.',\n",
              " 'parish.',\n",
              " 'weeks.',\n",
              " 'xxv.',\n",
              " 'owner',\n",
              " 'doing.',\n",
              " 'insatiable',\n",
              " 'yes--it',\n",
              " 'acquit',\n",
              " 'invite',\n",
              " 'aspect,',\n",
              " 'error',\n",
              " 'thinking',\n",
              " 'tedious',\n",
              " 'unpleasing,',\n",
              " 'allude,',\n",
              " 'glazing',\n",
              " 'opportunity',\n",
              " 'intermediate',\n",
              " 'admittedly',\n",
              " 'saying',\n",
              " 'truth.',\n",
              " 'contrive',\n",
              " 'rest.',\n",
              " 'open;',\n",
              " 'smile;',\n",
              " 'carefully',\n",
              " 'suffrages',\n",
              " 'unabated,',\n",
              " 'anyhow',\n",
              " 'v',\n",
              " 'lasted',\n",
              " 'if',\n",
              " 'unnecessary',\n",
              " 'hardest',\n",
              " 'talking',\n",
              " 'begun',\n",
              " 'allowing',\n",
              " 'society,',\n",
              " 'parted:',\n",
              " '_division',\n",
              " 'storing',\n",
              " 'points,',\n",
              " 'dared.',\n",
              " 'received!',\n",
              " 'invention',\n",
              " 'ill-founded,',\n",
              " 'give',\n",
              " 'happy.',\n",
              " 'alteration,',\n",
              " 'pertness,',\n",
              " 'tongue.',\n",
              " 'civilities.',\n",
              " 'defined.',\n",
              " 'thin,',\n",
              " 'compare',\n",
              " 'aversion',\n",
              " 'pleasantry.',\n",
              " 'provision',\n",
              " 'anyone,',\n",
              " 'paying',\n",
              " 'beginning,',\n",
              " 'complaints',\n",
              " 'dispense.',\n",
              " 'donations',\n",
              " 'irritable,',\n",
              " 'revenge',\n",
              " 'adjusting',\n",
              " 'perhaps',\n",
              " 'blameless',\n",
              " 'seizing',\n",
              " 'gives,',\n",
              " 'summer.',\n",
              " 'singular.',\n",
              " 'observations',\n",
              " 'improve',\n",
              " 'leigh',\n",
              " 'party',\n",
              " 'relation',\n",
              " 'blue',\n",
              " 'minuteness',\n",
              " 'determination',\n",
              " 'purpose;',\n",
              " 'relations;',\n",
              " 'extremely',\n",
              " 'everybody’s',\n",
              " 'travelling,',\n",
              " 'confessed',\n",
              " 'mistaken--or,',\n",
              " 'mud,',\n",
              " 'decisive,',\n",
              " 'sends',\n",
              " 'recollections.',\n",
              " 'conceited',\n",
              " 'foundations',\n",
              " 'hopes;',\n",
              " 'jumping',\n",
              " 'approach:',\n",
              " 'regarded',\n",
              " 'misfortunes',\n",
              " '(www.gutenberg.org),',\n",
              " 'pliancy',\n",
              " 'received',\n",
              " 'together,',\n",
              " 'stretch',\n",
              " 'ill-judging',\n",
              " 'divided',\n",
              " 'expected?',\n",
              " 'he;',\n",
              " 'of,',\n",
              " 'charity',\n",
              " 'distrust.',\n",
              " 'royalties',\n",
              " 'catherine,)',\n",
              " 'husbands.',\n",
              " 'tickets,',\n",
              " 'elaborate,',\n",
              " 'strength',\n",
              " 'existed.',\n",
              " 'card-table,',\n",
              " 'realized.',\n",
              " 'getting',\n",
              " 'besides,',\n",
              " 'team']"
            ]
          },
          "metadata": {},
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word mapping\n",
        "\n",
        "word_to_index = {word: i for i, word in enumerate(words)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}"
      ],
      "metadata": {
        "id": "9HpPKAxCC0jZ"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dict(list(word_to_index.items())[:50]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28dbLB4DDmPQ",
        "outputId": "2f8e2997-6fd7-48a9-b0e7-1de9a994865f"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'be--if': 0, 'half-reluctant': 1, 'blushed,': 2, 'ago.': 3, 'patronage,': 4, 'obsolete,': 5, '_are_': 6, 'add.': 7, 'battled': 8, 'demands': 9, 'peep': 10, '_knew_': 11, '(trademark/copyright)': 12, 'accompanying': 13, 'softness': 14, 'motive.': 15, 'paining': 16, 'performed': 17, 'bingleys’': 18, 'success,--no': 19, 'grieved.': 20, 'display.': 21, 'adds,': 22, 'enabling': 23, 'displaying': 24, 'ay,': 25, 'afflicting': 26, 'excelled': 27, 'insensible': 28, 'for,': 29, 'support.': 30, 'unnecessary.': 31, 'lament': 32, 'inhuman': 33, 'less,': 34, 'denying': 35, 'edmund': 36, 'earthly': 37, 'animating': 38, 'hurrying': 39, 'thoughtless': 40, 'refused?': 41, 'otherwise,': 42, 'lane,': 43, 'resist': 44, 'plain,': 45, 'remark;': 46, 'fine,--with': 47, 'as,': 48, 'neighbourhood.': 49}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create input 시퀀스, 레이블\n",
        "\n",
        "max_len = 10\n",
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in range(0,len(words) - max_len):\n",
        "  X.append([word_to_index[word] for word in words[i:i+max_len]])\n",
        "  y.append(word_to_index[words[i+max_len]])\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.long)\n",
        "y = torch.tensor(y)"
      ],
      "metadata": {
        "id": "jwPHGgDdBzmV"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkG2T07GE72y",
        "outputId": "fc80faa4-45dd-45fc-c7b9-0c1111cbf344"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8kYE9X4FuCK",
        "outputId": "22b922ce-1d52-4b29-aca3-474693f5be60"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13137, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FwIINdIF0QC",
        "outputId": "b73c0523-1d07-4c7b-9deb-5e8c7f36c803"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13137])"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # weight & bias 초기화\n",
        "\n",
        "        # ex) W_ii : 현재 input x_t와 곱해지는 weight, W_hi : 이전 hidden state h_(t-1)와 곱해지는 weight\n",
        "\n",
        "        # input gate\n",
        "        self.W_ii = nn.Parameter(torch.Tensor(embedding_dim, hidden_size))\n",
        "        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_ii = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hi = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # forget gate\n",
        "        self.W_if = nn.Parameter(torch.Tensor(embedding_dim, hidden_size))\n",
        "        self.W_hf = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_if = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hf = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # cell state update\n",
        "        self.W_ig = nn.Parameter(torch.Tensor(embedding_dim, hidden_size))\n",
        "        self.W_hg = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_ig = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_hg = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "        # output gate\n",
        "        self.W_io = nn.Parameter(torch.Tensor(embedding_dim, hidden_size))\n",
        "        self.W_ho = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
        "        self.b_io = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.b_ho = nn.Parameter(torch.Tensor(hidden_size))\n",
        "\n",
        "\n",
        "        # initialize\n",
        "        self.reset_parameters()\n",
        "\n",
        "\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Xavier 초기화\n",
        "        nn.init.xavier_uniform_(self.W_ii)\n",
        "        nn.init.xavier_uniform_(self.W_hi)\n",
        "        nn.init.constant_(self.b_ii, 0)\n",
        "        nn.init.constant_(self.b_hi, 0)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_if)\n",
        "        nn.init.xavier_uniform_(self.W_hf)\n",
        "        nn.init.constant_(self.b_if, 0)\n",
        "        nn.init.constant_(self.b_hf, 0)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_ig)\n",
        "        nn.init.xavier_uniform_(self.W_hg)\n",
        "        nn.init.constant_(self.b_ig, 0)\n",
        "        nn.init.constant_(self.b_hg, 0)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.W_io)\n",
        "        nn.init.xavier_uniform_(self.W_ho)\n",
        "        nn.init.constant_(self.b_io, 0)\n",
        "        nn.init.constant_(self.b_ho, 0)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        embedded = self.embedding(input) # hx은 이전 time step에서의 hidden state\n",
        "        if hx is None:\n",
        "            hx = torch.zeros(embedded.size(0), self.hidden_size, device=embedded.device)\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(embedded.size(1)):\n",
        "            hx = self.cell(embedded[:, i], hx)\n",
        "            outputs.append(hx)\n",
        "        outputs = torch.stack(outputs, dim=1)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def cell (self, input, hx):\n",
        "\n",
        "      # input gate\n",
        "      i_t = torch.sigmoid(torch.matmul(input, self.W_ii) + self.b_ii + torch.matmul(hx, self.W_hi) + self.b_hi)\n",
        "\n",
        "      # forget gate\n",
        "      f_t = torch.sigmoid(torch.matmul(input, self.W_if) + self.b_if + torch.matmul(hx, self.W_hf) + self.b_hf)\n",
        "\n",
        "      # cell state update\n",
        "      g_t = torch.tanh(torch.matmul(input, self.W_ig) + self.b_ig + torch.matmul(hx, self.W_hg) + self.b_hg)\n",
        "\n",
        "      c_t = f_t * hx + i_t * g_t\n",
        "\n",
        "      # output gate\n",
        "      o_t = torch.sigmoid(torch.matmul(input, self.W_io) + self.b_io + torch.matmul(hx, self.W_ho) + self.b_ho)\n",
        "\n",
        "      # hidden state update\n",
        "      h_t = o_t * torch.tanh(c_t)\n",
        "\n",
        "      return h_t"
      ],
      "metadata": {
        "id": "XUnjRlzhF438"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LSTM model\n",
        "\n",
        "vocab_size = len(word_to_index)  # 어휘 사전의 크기\n",
        "embedding_dim = 256  # 임베딩 차원\n",
        "hidden_size = 512\n",
        "model = LSTM(vocab_size, embedding_dim, hidden_size)"
      ],
      "metadata": {
        "id": "QXfH4pUXMfpz"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ck0TXDZIM5zV"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjZiB0ezOgB_",
        "outputId": "73da7572-da6b-4d40-82a0-c6c0d373879f"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13137, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI7YTC3bQerO",
        "outputId": "f5484452-5db1-4c63-bf25-453925a1075e"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([13137])"
            ]
          },
          "metadata": {},
          "execution_count": 251
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"어휘 사전 크기: {len(word_to_index)}\")\n",
        "print(f\"모델 임베딩 계층 크기: {model.embedding.embedding_dim}\")\n",
        "\n",
        "# 입력 데이터에서 최소 인덱스와 최대 인덱스 확인\n",
        "min_index = min([min(seq) for seq in X])  # X는 입력 데이터\n",
        "max_index = max([max(seq) for seq in X])\n",
        "\n",
        "print(f\"최소 인덱스: {min_index}, 최대 인덱스: {max_index}\")\n",
        "\n",
        "# word_to_index의 최대 인덱스 값을 확인\n",
        "max_vocab_index = max(word_to_index.values())\n",
        "\n",
        "if max_index > max_vocab_index:\n",
        "    print(\"입력 데이터에 어휘 사전에 없는 인덱스가 포함되어 있습니다.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgxvSFjzYUu7",
        "outputId": "cf410f5d-3275-44d0-ff02-46c20d8584d8"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어휘 사전 크기: 13147\n",
            "모델 임베딩 계층 크기: 256\n",
            "최소 인덱스: 0, 최대 인덱스: 13145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습\n",
        "def train(model, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        total_loss = 0\n",
        "        for i in range(0, X.size(0)):\n",
        "            inputs = X[i].unsqueeze(0)  # 배치 차원 추가\n",
        "            targets = y[i].unsqueeze(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[:, -1, :], targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / X.size(0)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{epochs} Loss: {avg_loss:.4f} Time: {elapsed_time:.2f}s')\n",
        "\n",
        "train(model, criterion, optimizer, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "KVl7crktNAh0",
        "outputId": "f5a7b91c-55df-49d9-b214-cb80804527a1"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Target 512 is out of bounds.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-253-f2dc0d9dc77f>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Epoch {epoch+1}/{epochs} Loss: {avg_loss:.4f} Time: {elapsed_time:.2f}s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-253-f2dc0d9dc77f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1180\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Target 512 is out of bounds."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict next word function\n",
        "\n",
        "def predict_next_word(sentence):\n",
        "    tokens = sentence.lower().split()\n",
        "    tokenized_sentence = [word_to_index[word] for word in tokens]\n",
        "    tokenized_sentence = torch.tensor(tokenized_sentence).unsqueeze(0)\n",
        "    output = model(tokenized_sentence)\n",
        "    _, predicted_index = torch.max(output[:, -1, :], 1)\n",
        "    predicted_word = index_to_word[predicted_index.item()]\n",
        "\n",
        "    return predicted_word"
      ],
      "metadata": {
        "id": "ednvaJrUNIzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# test\n",
        "\n",
        "text = \"who are the objects of the personal affection\"\n",
        "num_predictions =\n",
        "\n",
        "for i in range(num_predictions):\n",
        "    # 텍스트 토큰화\n",
        "    tokens = text.split()\n",
        "    token_text = [word_to_index[word] for word in tokens]\n",
        "\n",
        "    # 패딩\n",
        "    padded_token_text = token_text[-max_len:]  # 최대 길이에 맞게 자름\n",
        "    padded_token_text = torch.tensor(padded_token_text).unsqueeze(0)\n",
        "\n",
        "    # 모델로부터 다음 단어 예측\n",
        "    output = model(padded_token_text)\n",
        "    probabilities = F.softmax(output, dim=1)\n",
        "    _, predicted_index = torch.max(probabilities, 1)\n",
        "    predicted_word = index_to_word[predicted_index.item()]\n",
        "\n",
        "    # 다음 단어 추가\n",
        "    text += \" \" + predicted_word\n",
        "    print(text)\n",
        "    time.sleep(2)"
      ],
      "metadata": {
        "id": "CsyZuc2hNOb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}